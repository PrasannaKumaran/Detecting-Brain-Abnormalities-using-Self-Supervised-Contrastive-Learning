{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHXE8J-EPmxX"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from imutils import paths\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "tqdm.pandas()\n",
        "le = LabelEncoder()\n",
        "np.random.seed(666)\n",
        "tf.random.set_seed(666)\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Define Augmentation Class to augment image slices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iDKbbylVRX2"
      },
      "outputs": [],
      "source": [
        "class AugmentSlices(object):\n",
        "    def __call__(self, sample):        \n",
        "        # Random flips\n",
        "        sample = self._random_apply(tf.image.flip_left_right, sample, p=0.5)\n",
        "        # Randomly apply transformation (color distortions) with probability p.\n",
        "        sample = self._random_apply(self._color_jitter, sample, p=0.8)\n",
        "        sample = self._random_apply(self._color_drop, sample, p=0.2)\n",
        "        return sample\n",
        "\n",
        "    def _color_jitter(self, x, s=1):\n",
        "        # one can also shuffle the order of following augmentations\n",
        "        # each time they are applied.\n",
        "        x = tf.image.random_brightness(x, max_delta=0.8*s)\n",
        "        x = tf.image.random_contrast(x, lower=1-0.8*s, upper=1+0.8*s)\n",
        "        x = tf.image.random_saturation(x, lower=1-0.8*s, upper=1+0.8*s)\n",
        "        x = tf.image.random_hue(x, max_delta=0.2*s)\n",
        "        x = tf.clip_by_value(x, 0, 1)\n",
        "        return x\n",
        "    \n",
        "    def _color_drop(self, x):\n",
        "        x = tf.image.rgb_to_grayscale(x)\n",
        "        x = tf.tile(x, [1, 1, 1, 3])\n",
        "        return x\n",
        "    \n",
        "    def _random_apply(self, func, x, p):\n",
        "        return tf.cond(\n",
        "          tf.less(tf.random.uniform([], minval=0, maxval=1, dtype=tf.float32),\n",
        "                  tf.cast(p, tf.float32)),\n",
        "          lambda: func(x),\n",
        "          lambda: x)\n",
        "\n",
        "# Build the augmentation pipeline\n",
        "data_augmentation = Sequential([Lambda(AugmentSlices())])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Implementation of Gaussian Blur "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GaussianBlur(object):\n",
        "    def __init__(self, kernel_size, min=0.1, max=2.0):\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "        # kernel size is set to be 10% of the image height/width\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        sample = np.array(sample)\n",
        "        # blur the image with a 50% chance\n",
        "        prob = np.random.random_sample()\n",
        "        if prob < 0.5:\n",
        "            sigma = (self.max - self.min) * np.random.random_sample() + self.min\n",
        "            sample = cv2.GaussianBlur(sample, (self.kernel_size, self.kernel_size), sigma)\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Image preprocessing utils\n",
        "* Parse and process image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JdZgMjLVe16"
      },
      "outputs": [],
      "source": [
        "\n",
        "def parse_images(image_path):\n",
        "    image_string = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = tf.image.resize(image, size=[224, 224])\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Create Custom TensorFlow dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0DKxZ36VgCG"
      },
      "outputs": [],
      "source": [
        "train_images = list(paths.list_images(\"/content/gdrive/MyDrive/FYP/SSL_BinaryData/train\"))\n",
        "BATCH_SIZE = 16\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(train_images)\n",
        "train_ds = (\n",
        "    train_ds\n",
        "    .map(parse_images, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    .shuffle(1024)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Defining the Architecture for the pretext task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIA8QWV8Vjnm"
      },
      "outputs": [],
      "source": [
        "def get_resnet_simclr(hidden_1, hidden_2, hidden_3):\n",
        "    base_model = tf.keras.applications.ResNet50(include_top=False, weights=None, input_shape=(224, 224, 3))\n",
        "    base_model.trainable = True\n",
        "    inputs = Input((224, 224, 3))\n",
        "    h = base_model(inputs, training=True)\n",
        "    h = GlobalAveragePooling2D()(h)\n",
        "\n",
        "    projection_1 = Dense(hidden_1)(h)\n",
        "    projection_1 = Activation(\"relu\")(projection_1)\n",
        "    projection_2 = Dense(hidden_2)(projection_1)\n",
        "    projection_2 = Activation(\"relu\")(projection_2)\n",
        "    projection_3 = Dense(hidden_3)(projection_2)\n",
        "    resnet_simclr = Model(inputs, projection_3)\n",
        "    return resnet_simclr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zbj6E6hdVl0s"
      },
      "outputs": [],
      "source": [
        "def get_negative_mask(batch_size):\n",
        "    '''\n",
        "    Return a mask that removes the similarity score of equal/similar images.\n",
        "    This function ensures that only distinct pair of images get their similarity scores passed as negative examples\n",
        "    '''\n",
        "    negative_mask = np.ones((batch_size, 2 * batch_size), dtype=bool)\n",
        "    for i in range(batch_size):\n",
        "        negative_mask[i, i] = 0\n",
        "        negative_mask[i, i + batch_size] = 0\n",
        "    return tf.constant(negative_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Defining Functions to calculate similarity values between vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cosine_sim_1d = tf.keras.losses.CosineSimilarity(axis=1, reduction=tf.keras.losses.Reduction.NONE)\n",
        "cosine_sim_2d = tf.keras.losses.CosineSimilarity(axis=2, reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "# Mask to remove positive examples from the batch of negative samples\n",
        "negative_mask = get_negative_mask(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _cosine_simililarity_dim1(x, y):\n",
        "    v = cosine_sim_1d(x, y)\n",
        "    return v\n",
        "\n",
        "def _cosine_simililarity_dim2(x, y):\n",
        "    '''\n",
        "    x shape: (N, 1, C)\n",
        "    y shape: (1, 2N, C)\n",
        "    v shape: (N, 2N)\n",
        "    '''\n",
        "    v = cosine_sim_2d(tf.expand_dims(x, 1), tf.expand_dims(y, 0))\n",
        "    return v\n",
        "\n",
        "def _dot_simililarity_dim1(x, y):\n",
        "    '''\n",
        "    x shape: (N, 1, C)\n",
        "    y shape: (N, C, 1)\n",
        "    v shape: (N, 1, 1)\n",
        "    '''\n",
        "    v = tf.matmul(tf.expand_dims(x, 1), tf.expand_dims(y, 2))\n",
        "    return v\n",
        "\n",
        "def _dot_simililarity_dim2(x, y):\n",
        "    '''\n",
        "    x shape: (N, 1, C)\n",
        "    y shape: (1, C, 2N)\n",
        "    v shape: (N, 2N)\n",
        "    '''\n",
        "    v = tf.tensordot(tf.expand_dims(x, 1), tf.expand_dims(tf.transpose(y), 0), axes=2)\n",
        "    return v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Defining the train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzZUqIerWB6X"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(xis, xjs, model, optimizer, criterion, temperature):\n",
        "    with tf.GradientTape() as tape:\n",
        "        zis = model(xis)\n",
        "        zjs = model(xjs)\n",
        "        # normalize projection feature vectors\n",
        "        zis = tf.math.l2_normalize(zis, axis=1)\n",
        "        zjs = tf.math.l2_normalize(zjs, axis=1)\n",
        "        l_pos = _dot_simililarity_dim1(zis, zjs)\n",
        "        l_pos = tf.reshape(l_pos, (BATCH_SIZE, 1))\n",
        "        l_pos /= temperature\n",
        "\n",
        "        negatives = tf.concat([zjs, zis], axis=0)\n",
        "        loss = 0\n",
        "        for positives in [zis, zjs]:\n",
        "            l_neg = _dot_simililarity_dim2(positives, negatives)\n",
        "            labels = tf.zeros(BATCH_SIZE, dtype=tf.int32)\n",
        "            l_neg = tf.boolean_mask(l_neg, negative_mask)\n",
        "            l_neg = tf.reshape(l_neg, (BATCH_SIZE, -1))\n",
        "            l_neg /= temperature\n",
        "            logits = tf.concat([l_pos, l_neg], axis=1) \n",
        "            loss += criterion(y_pred=logits, y_true=labels)\n",
        "        loss = loss / (2 * BATCH_SIZE)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tPZYm6lWGiA"
      },
      "outputs": [],
      "source": [
        "def train_simclr(model, dataset, optimizer, criterion, temperature = 0.1, epochs = 100):\n",
        "    step_wise_loss = []\n",
        "    epoch_wise_loss = []\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        for image_batch in dataset:\n",
        "            a = data_augmentation(image_batch)\n",
        "            b = data_augmentation(image_batch)\n",
        "            loss = train_step(a, b, model, optimizer, criterion, temperature)\n",
        "            step_wise_loss.append(loss)\n",
        "        epoch_wise_loss.append(np.mean(step_wise_loss))\n",
        "        if epoch % 10 == 0:\n",
        "            print(\"epoch: {} loss: {:.3f}\".format(epoch + 1, np.mean(step_wise_loss)))\n",
        "    return epoch_wise_loss, model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Defining the Hyperparameter settings, loss functions, Decays, Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "VJytxjsiWL5e",
        "outputId": "d5d3b753-fc38-4487-cc76-ee781be00a49"
      },
      "outputs": [],
      "source": [
        "criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = tf.keras.losses.Reduction.SUM)\n",
        "decay_steps = 1000\n",
        "lr_decayed_fn = tf.keras.experimental.CosineDecay(initial_learning_rate = 0.1, decay_steps = decay_steps)\n",
        "optimizer = tf.keras.optimizers.SGD(lr_decayed_fn)\n",
        "resnet_simclr_2 = get_resnet_simclr(256, 128, 50)\n",
        "epoch_wise_loss, resnet_simclr  = train_simclr(resnet_simclr_2, train_ds, optimizer, criterion, temperature = 0.1, epochs = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Plotting Training Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with plt.xkcd():\n",
        "    plt.plot(epoch_wise_loss)\n",
        "    plt.title(\"tau = 0.1, h1 = 256, h2 = 128, h3 = 50\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Storing Model Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVbhL79oM_TP"
      },
      "outputs": [],
      "source": [
        "weights_folder = '/content/gdrive/MyDrive/FYP' + \"_resnet_simclr.h5\"\n",
        "resnet_simclr.save_weights(weights_folder)\n",
        "simclr_weights = resnet_simclr.weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Testing (Downstream Task)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bLdP27Qa6P0",
        "outputId": "b206f624-f2ad-403c-a877-cb7bf1717905"
      },
      "outputs": [],
      "source": [
        "# Train and test image paths\n",
        "train_images = list(paths.list_images(\"/content/gdrive/MyDrive/FYP/SSL_BinaryData/train\"))\n",
        "test_images = list(paths.list_images(\"/content/gdrive/MyDrive/FYP/SSL_BinaryData/test\"))\n",
        "print(len(train_images), len(test_images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1wOmnWnbIBz",
        "outputId": "0aec6220-3bbf-4a73-d24a-41843f3b7fff"
      },
      "outputs": [],
      "source": [
        "# 10% of the dataset\n",
        "train_images_10 = np.random.choice(train_images, len(train_images)//10)\n",
        "len(train_images_10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKE7xH9zbKDU"
      },
      "outputs": [],
      "source": [
        "def prepare_images(image_paths):\n",
        "    '''\n",
        "    Prepare the image for modelling\n",
        "    '''\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for image in tqdm(image_paths):\n",
        "        image_pixels = tf.io.read_file(image)\n",
        "        image_pixels = tf.image.decode_jpeg(image_pixels, channels=3)\n",
        "        image_pixels = tf.image.convert_image_dtype(image_pixels, tf.float32)\n",
        "        image_pixels = tf.image.resize(image_pixels, size=[224, 224])\n",
        "        label = image.split(\"/\")[-1].split(\"_\")[0]\n",
        "        images.append(image_pixels)\n",
        "        labels.append(label)\n",
        "\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "    return images, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CM04Ju6bWc9",
        "outputId": "98a4157d-8fb1-4bfb-ea0a-1217cbb47373"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = prepare_images(train_images_10)\n",
        "X_test, y_test = prepare_images(test_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Label Encoding the labels (abnormal/normal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUnFS7Q3bXx2"
      },
      "outputs": [],
      "source": [
        "y_train_enc = le.fit_transform(y_train)\n",
        "y_test_enc = le.transform(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Loading Weights of the model from the pre-text task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J7riveBbqo0",
        "outputId": "4030d943-e6a8-4442-aea4-b05aaf7494a8"
      },
      "outputs": [],
      "source": [
        "resnet_simclr = get_resnet_simclr(256, 128, 50)\n",
        "resnet_simclr.load_weights('/content/20220417-163415_resnet_simclr.h5')\n",
        "resnet_simclr.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Plotting Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjPeJ95_btcJ"
      },
      "outputs": [],
      "source": [
        "def plot_training(H):\n",
        "\twith plt.xkcd():\n",
        "\t\tplt.plot(H.history[\"loss\"], label=\"train_loss\")\n",
        "\t\tplt.plot(H.history[\"val_loss\"], label=\"val_loss\")\n",
        "\t\tplt.plot(H.history[\"accuracy\"], label=\"train_acc\")\n",
        "\t\tplt.plot(H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "\t\tplt.title(\"Training Loss and Accuracy\")\n",
        "\t\tplt.xlabel(\"Epoch #\")\n",
        "\t\tplt.ylabel(\"Loss/Accuracy\")\n",
        "\t\tplt.legend(loc=\"lower left\")\n",
        "\t\tplt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaven5Gid22y"
      },
      "outputs": [],
      "source": [
        "def get_linear_model(features):\n",
        "    '''\n",
        "    Obtain the Linear layer for Downstream task\n",
        "    '''\n",
        "    linear_model = Sequential([Dense(2, input_shape = (features, ), activation = \"softmax\")])\n",
        "    return linear_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOd10sQtd5N-",
        "outputId": "3337d71e-8314-451e-cd6c-8e10f6eda649"
      },
      "outputs": [],
      "source": [
        "resnet_simclr.layers[1].trainable = False\n",
        "resnet_simclr.summary()\n",
        "\n",
        "# Encoder model with non-linear projections\n",
        "projection = Model(resnet_simclr.input, resnet_simclr.layers[-2].output)\n",
        "# Extract train and test features\n",
        "train_features = projection.predict(X_train)\n",
        "test_features = projection.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Training model for Downstream task and Plotting the Train Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AQdHYmGyeD3q",
        "outputId": "75b91fd4-b8e2-44ec-99cb-3171402d6f4c"
      },
      "outputs": [],
      "source": [
        "# Early Stopping to prevent overfitting\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience = 2, verbose = 2, restore_best_weights=True)\n",
        "# Linear model\n",
        "linear_model = get_linear_model(128)\n",
        "linear_model.compile(\n",
        "    loss = \"sparse_categorical_crossentropy\",\n",
        "    metrics = [\"accuracy\"],\n",
        "    optimizer = \"adam\"\n",
        "    )\n",
        "history = linear_model.fit(\n",
        "    train_features, \n",
        "    y_train_enc,\n",
        "    validation_data=(test_features, y_test_enc),\n",
        "    batch_size = 64,\n",
        "    epochs = 35\n",
        ")\n",
        "plot_training(history)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "SSL_Implementation",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
