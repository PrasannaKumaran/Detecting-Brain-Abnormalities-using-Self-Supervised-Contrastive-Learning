{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsUSD430CNaI",
        "outputId": "07d9c15c-7f7b-4822-aee2-03e20557b5ad"
      },
      "outputs": [],
      "source": [
        "#!pip install dipy\n",
        "#!pip install pylibjpeg pylibjpeg-libjpeg pydicom \n",
        "#!pip install GDCM \n",
        "#!pip install gdcm\n",
        "#!pip install python-gdcm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "NuvRqYVVo0iu",
        "outputId": "26cbc4e1-9a15-46e6-e2c6-e57b40a27047"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import glob\n",
        "import pydicom\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "from tqdm import tqdm\n",
        "import matplotlib.image as mpimg\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "tqdm.pandas()\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Extracting and storing data for binary (abnormal/normal) classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcvEZ_HgKust"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/gdrive/MyDrive/FYP/binary_classification_data.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/gdrive/MyDrive/FYP/binary_classification_data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23cwi2SYpVZN"
      },
      "source": [
        "#### Identify the subfolders to be considered for experimentation\n",
        "* The below code extracts the subfolders that have 5mm cross sections for each patient\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXc3CzRZsIMM",
        "outputId": "621e9589-bcf0-4415-ddce-3cf7ae08c10d"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content/gdrive/MyDrive/qc500'\n",
        "stored_folders = []\n",
        "for dir_entry in tqdm(os.scandir(path = data_dir)):\n",
        "    subject_dir_dict = dict()\n",
        "    if dir_entry.is_dir():\n",
        "        combined_name = ''.join(dir_entry.name.split('-'))\n",
        "        subject_dir_dict[dir_entry.name] = list()\n",
        "        for sub_dir in os.scandir(dir_entry.path + f'/{combined_name} {combined_name}' + \"/Unknown Study/\"):\n",
        "            if sub_dir.is_dir() and not re.search(\"(POST|(?<!PRE )CONTRAST|BONE)\", sub_dir.name):\n",
        "                dcm_files = glob.glob(sub_dir.path + \"/*.dcm\")\n",
        "                dicom_data = pydicom.read_file(dcm_files[0])\n",
        "                sub_info = [sub_dir.name, float(dicom_data.PixelSpacing[0]), float(dicom_data.PixelSpacing[1]), float(dicom_data.SliceThickness), len(dcm_files)]\n",
        "                subject_dir_dict[dir_entry.name].append(sub_info)\n",
        "\n",
        "        if(len(subject_dir_dict[dir_entry.name]) < 1):\n",
        "            print(\"Warning: {subject} found no usable subdirectories\".format(subject=dir_entry.name))\n",
        "            subject_dir_dict.pop(dir_entry.name)    # remove the entry from the dict\n",
        "\n",
        "        elif(len(subject_dir_dict[dir_entry.name]) > 1):\n",
        "            subject_scans = subject_dir_dict[dir_entry.name]\n",
        "            scans_info = np.array(subject_scans)\n",
        "            sorted_idx = np.argsort(scans_info[:, 3])\n",
        "            # check for a 5mm slice scan\n",
        "            int(scans_info[sorted_idx[-1], 3].astype(float))\n",
        "            if((scans_info[sorted_idx[-1], 3].astype(float)) == 5.0):\n",
        "                scans_info_5 = scans_info[(scans_info[:, 3].astype(float)) == 5.0]\n",
        "                # check if there are two 5mm slice scans\n",
        "                if(float(scans_info[sorted_idx[-2], 3]) == 5.0):\n",
        "                    # if more than one 5mm slice scan, use the study with fewest slices\n",
        "                    sorted_slices_idx = np.argsort(scans_info[:, 4])\n",
        "                    subject_dir_dict[dir_entry.name] = subject_dir_dict[dir_entry.name][sorted_slices_idx[-1]]\n",
        "                    if(subject_dir_dict[dir_entry.name][4] < 32):\n",
        "                        print(\"Warning: Fewer than 32 slices for {}\".format(dir_entry.path + \"/\" + subject_dir_dict[dir_entry.name][0]))\n",
        "                else:\n",
        "                    subject_dir_dict[dir_entry.name] = subject_dir_dict[dir_entry.name][sorted_idx[-1]]         # replace entries with the 5mm study\n",
        "\n",
        "            else:   # no 5mm slice scans, so choose the smallest\n",
        "                subject_dir_dict[dir_entry.name] = subject_dir_dict[dir_entry.name][sorted_idx[0]]     # replace with the small slice scan\n",
        "    \n",
        "    for key, value in subject_dir_dict.items():\n",
        "      combined_name = ''.join(key.split('-'))\n",
        "      if isinstance(value[0], str):\n",
        "        stored_folders.append(f'{key}/{combined_name} {combined_name}/Unknown Study/{value[0]}')\n",
        "      else:\n",
        "        stored_folders.append(f'{key}/{combined_name} {combined_name}/Unknown Study/{value[0][0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Store the list of subfolders in a text file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsHn_k5dAUSB"
      },
      "outputs": [],
      "source": [
        "file=open('/content/gdrive/MyDrive/FYP/important_folders.txt','w')\n",
        "for items in stored_folders:\n",
        "    file.writelines(items+'\\n')\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D_ujmWbp51l"
      },
      "source": [
        "#### To understand about the distribution of subfolders in the dataset.\n",
        "* Eg. Number of subfolders with the name CT PLAIN THIN, CT 5mm PRECONTRAST THIN etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltUTUlXs0KhK"
      },
      "outputs": [],
      "source": [
        "splits_ = dict()\n",
        "for file in stored_folders:\n",
        "  name = file.split('/')[-1]\n",
        "  if name in splits_:\n",
        "    splits_[name] += 1\n",
        "  else:\n",
        "    splits_[name] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2okwrU_XqOhH"
      },
      "source": [
        "#### Extracting the metadata from all slices present in the dataset\n",
        "* The extracted dataset is used to merge with the slices where annotations are available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYVHZLFtkTDw"
      },
      "outputs": [],
      "source": [
        "meta_cols = [\n",
        "             'Rows',\n",
        "             'Columns',\n",
        "             'InstanceNumber',\n",
        "             'SOPInstanceUID',\n",
        "             'PatientID',\n",
        "             'SeriesInstanceUID',\n",
        "             'StudyInstanceUID',\n",
        "             'ScanOptions'\n",
        "             ] # required metadata information\n",
        "information = []\n",
        "qc_path = '/content/gdrive/MyDrive/qc500/'\n",
        "for path, subdirs, files in tqdm(os.walk(qc_path)):\n",
        "    for name in files: \n",
        "        current_dict = {}\n",
        "        current_dict = {col: \"\" for col in meta_cols}\n",
        "        current_dict['path'] = \"\"\n",
        "        current_dict['subfolder'] = \"\"  \n",
        "        if 'dcm' in name:\n",
        "          dicom_object = pydicom.dcmread(os.path.join(path, name))\n",
        "          for col in meta_cols: \n",
        "            current_dict[col] =  str(getattr(dicom_object, col))\n",
        "          current_dict['path'] = path\n",
        "          current_dict['subfolder'] = path.split('/')[-1]\n",
        "          information.append(current_dict)    \n",
        "\n",
        "df = pd.DataFrame.from_dict(information)\n",
        "df.to_csv('/content/gdrive/MyDrive/FYP/data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5mKDYrhq3hf"
      },
      "source": [
        "##### Analysing the data where annotations for the slices are available. There are 3 variants for the annotations and the link to download these annotations is https://physionet.org/content/bhx-brain-bounding-box/1.1/ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxwThaTjmoTR",
        "outputId": "80625dd8-aa54-42b9-9baa-7719e727736a"
      },
      "outputs": [],
      "source": [
        "og_bounding = pd.read_csv('/content/gdrive/MyDrive/FYP/1_Initial_Manual_Labeling.csv')\n",
        "selected_bounding = pd.read_csv('/content/gdrive/MyDrive/FYP/3_Extrapolation_to_Selected_Series.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO_ogpSvQyBV"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/FYP/data.csv')\n",
        "data_with_bounding_selected = pd.merge(df, selected_bounding, on=['SOPInstanceUID', 'StudyInstanceUID', 'SeriesInstanceUID'], how='inner')\n",
        "\n",
        "# Storing the list of patients with hemorrhage \n",
        "abnormal_haemo = list(set(data_with_bounding_selected['PatientID']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJ9OBAh3C-9M"
      },
      "outputs": [],
      "source": [
        "with open('/content/gdrive/MyDrive/FYP/abnormal_hemorrhage.pkl', 'wb') as f:\n",
        "  pkl.dump(abnormal_haemo, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo0vt6b4D6ms",
        "outputId": "95826482-6908-4459-da37-2e8bb4fc79a5"
      },
      "outputs": [],
      "source": [
        "# abnormal_scans_list contains the list of patient with one or more abnormality according to the 3 radiologists who analysed the results\n",
        "with open('/content/gdrive/MyDrive/FYP/abnormal_scans_list.pkl', 'rb') as f:\n",
        "  all_abnormal_scans = pkl.load(f)\n",
        "\n",
        "# Store the patients with abnormalities other than hemorrhages\n",
        "other_abnormalities = list(set(all_abnormal_scans) - set(abnormal_haemo ))\n",
        "with open('/content/gdrive/MyDrive/FYP/abnormal_others.pkl', 'wb') as f:\n",
        "  pkl.dump(other_abnormalities, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XefRGEPprTsA"
      },
      "source": [
        "##### Merging the data we obtained by parsing the dataset and the dataset where annotations (bounding box coordinates are available).\n",
        "* Merging datasets based on SOPInstanceID (slice), StudyInstanceID (Patient) and SeriesInstanceID (subfolder in our case)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk5zo9d0n1OU",
        "outputId": "8c21c232-a521-4f2d-f5d9-dcc784baf5ae"
      },
      "outputs": [],
      "source": [
        "data_with_bounding_original = pd.merge(df, og_bounding, on=['SOPInstanceUID', 'StudyInstanceUID', 'SeriesInstanceUID'], how='inner')\n",
        "for count, patient in enumerate(set(data_with_bounding_original['PatientID'])):\n",
        "  sample = data_with_bounding_original[data_with_bounding_original['PatientID'] == patient]\n",
        "  print(\"\\n\\nPatient \", patient, \"\\n\", sample['subfolder'].value_counts())\n",
        "print(\"Total number of patients\", count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-A9dzArio2h",
        "outputId": "dff8bab1-0291-42bf-9be5-c92b01d3e41a"
      },
      "outputs": [],
      "source": [
        "data_with_bounding_selected = pd.merge(df, selected_bounding, on=['SOPInstanceUID', 'StudyInstanceUID', 'SeriesInstanceUID'], how='inner')\n",
        "for count, patient in enumerate(set(data_with_bounding_selected['PatientID'])):\n",
        "  sub = data_with_bounding_selected[data_with_bounding_selected['PatientID'] == patient]\n",
        "  print(\"\\n\\nPatient \", patient, \"\\n\", sub['subfolder'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7YeWOqFrmyo"
      },
      "source": [
        "#### Testing the distribution / number of subfolders for a patient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCHBXHYRy19e",
        "outputId": "ce36fb28-535d-44c1-e6b2-7158b69b1565"
      },
      "outputs": [],
      "source": [
        "sample_sel_test = data_with_bounding_selected[data_with_bounding_selected['PatientID'] == 'CQ500-CT-284']\n",
        "sample_og_test = data_with_bounding_original[data_with_bounding_original['PatientID'] == 'CQ500-CT-284']\n",
        "print(\"Original : \", sample_og_test['subfolder'].value_counts())\n",
        "print(\"Extrapolated : \", sample_sel_test['subfolder'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AwHGv2GSDMM",
        "outputId": "26650c8e-25c1-4233-9086-9e49d3f64f14"
      },
      "outputs": [],
      "source": [
        "##### Checking dimensions of the slices\n",
        "df['Rows'].value_counts(), df['Columns'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVZFQDTRsQ3V"
      },
      "source": [
        "#### Extracting the required subfolders for each patient where localized annotations are available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9tGKHnnsP9y",
        "outputId": "3e942b58-2aac-4f8e-a62d-0f59bdd6f11b"
      },
      "outputs": [],
      "source": [
        "required_subs = {}\n",
        "patients = set(data_with_bounding_original['PatientID'])\n",
        "sample = data_with_bounding_original[data_with_bounding_original['PatientID'] == patient]\n",
        "sub = data_with_bounding_selected[data_with_bounding_selected['PatientID'] == patient]\n",
        "  \n",
        "for patient in tqdm(patients):\n",
        "  selected_box = data_with_bounding_selected[data_with_bounding_selected['PatientID'] == patient]\n",
        "  original_box = data_with_bounding_original[data_with_bounding_original['PatientID'] == patient]\n",
        "  selected_box_subs = set(selected_box['subfolder'].value_counts().keys())\n",
        "  original_box_subs = set(original_box['subfolder'].value_counts().keys())\n",
        "  if (len(original_box_subs) > 1) or (len(selected_box_subs) > 1):\n",
        "    required_folder = selected_box_subs - original_box_subs\n",
        "    required_subs[patient] = list(required_folder) \n",
        "  else:\n",
        "    required_subs[patient] = list(selected_box_subs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l07leqiCvu9b",
        "outputId": "d1722fc4-590e-4fac-a1b8-0b05981b3fcd"
      },
      "outputs": [],
      "source": [
        "list_of_subfolders = []\n",
        "for patient, folder in tqdm(required_subs.items()):\n",
        "  if folder:\n",
        "    sample = data_with_bounding_selected[(data_with_bounding_selected['PatientID'] == patient) & (data_with_bounding_selected['subfolder'] == folder[0])]\n",
        "    list_of_subfolders.append(sample['path'].values[0])  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpPwYARMvxC4"
      },
      "outputs": [],
      "source": [
        "# Store the list of subfolders that have localized annotations for slices in a text file\n",
        "file=open('/content/gdrive/MyDrive/FYP/hemorrhage_subfolders.txt','w')\n",
        "for items in list_of_subfolders:\n",
        "    file.writelines(items+'\\n')\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6kCAao_VaxW",
        "outputId": "db97e415-d6cc-45fb-8696-406ae664a8f0"
      },
      "outputs": [],
      "source": [
        "f = open('/content/gdrive/MyDrive/FYP/hemorrhage_subfolders.txt', 'r')\n",
        "lines = f.readlines()\n",
        "codes = [l.split('/')[5] for l in lines]\n",
        "\n",
        "missing = []\n",
        "path = '/content/gdrive/MyDrive/qc500'\n",
        "for fi in os.listdir(path):\n",
        "  if os.path.isdir(os.path.join(path, fi)):\n",
        "    if str(fi) not in codes:\n",
        "      missing.append(fi)\n",
        "print(\"# Missing : \", len(missing))\n",
        "\n",
        "# Store the list of subfolders that do not have localized annotations for slices in a text file\n",
        "file=open('/content/gdrive/MyDrive/FYP/non_hemorrhage.txt','w')\n",
        "for items in missing:\n",
        "    file.writelines(items+'\\n')\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N0aZk5K9eWN6",
        "outputId": "cd3b4edb-5ca4-4297-dbac-6f8163576c79"
      },
      "outputs": [],
      "source": [
        "# Contains the ground truth values provided by each of the radiologists\n",
        "reads = pd.read_csv('/content/gdrive/MyDrive/qc500/reads.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCG6CLJRjbzS",
        "outputId": "81d9b0cf-9c7a-444e-e0b7-7b8292bb4027"
      },
      "outputs": [],
      "source": [
        "file=open('/content/gdrive/MyDrive/FYP/important_folders.txt','r')\n",
        "subfolders = [x.strip() for x in file.readlines()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Storing subfolders information for normal scans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3EGfh3wHX_j",
        "outputId": "9aa4ded9-9044-44a9-bffa-7fdcfdfc339c"
      },
      "outputs": [],
      "source": [
        "with open('/content/gdrive/MyDrive/FYP/normal_scans_list.pkl', 'rb') as f:\n",
        "  normal_scans = pkl.load(f)\n",
        "for scan in tqdm(normal_scans):\n",
        "  for series in subfolders:\n",
        "    scan_no = series.split('/')[0]\n",
        "    if scan == scan_no:\n",
        "      source = os.path.join('/content/gdrive/MyDrive/qc500', series)\n",
        "      cur = series.split('/')[-1]\n",
        "      destination = os.path.join('/content/gdrive/MyDrive/FYP/Dataset/normal_scans', scan, cur)\n",
        "      shutil.copytree(source, destination)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Storing subfolders information for abnormal haemorrahge scans\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Raxi1-yyHfNE",
        "outputId": "e22c8cd8-259f-4346-d888-956d0458b9be"
      },
      "outputs": [],
      "source": [
        "with open('/content/gdrive/MyDrive/FYP/abnormal_haemo_paths.pkl', 'rb') as f:\n",
        "    selected_paths = pkl.load(f)\n",
        "with open('/content/gdrive/MyDrive/FYP/abnormal_hemorrhage.pkl', 'rb') as f:\n",
        "  abnormal_haemo = pkl.load(f)\n",
        "for scan in abnormal_haemo:\n",
        "  for series in selected_paths:\n",
        "    scan_no = series.split('/')[5]\n",
        "    if scan == scan_no:\n",
        "      source = os.path.join('/content/gdrive/MyDrive/qc500', series)\n",
        "      cur = series.split('/')[-1]\n",
        "      destination = os.path.join('/content/gdrive/MyDrive/FYP/Dataset/abnormal_hemorrhage', scan, cur)\n",
        "      shutil.copytree(source, destination)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Storing subfolder information for other abnormal (non-hemorrhage) scans\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtOI77lzKQo1",
        "outputId": "c39e3f0e-a00a-4d65-8282-a8a0cbdd0779"
      },
      "outputs": [],
      "source": [
        "with open('/content/gdrive/MyDrive/FYP/abnormal_others.pkl', 'rb') as f:\n",
        "  other_abnormalities = pkl.load(f)\n",
        "for scan in tqdm(other_abnormalities):\n",
        "  for series in subfolders:\n",
        "    scan_no = series.split('/')[0]\n",
        "    if scan == scan_no:\n",
        "      source = os.path.join('/content/gdrive/MyDrive/qc500', series)\n",
        "      cur = series.split('/')[-1]\n",
        "      destination = os.path.join('/content/gdrive/MyDrive/FYP/Dataset/abnormal_others', scan, cur)\n",
        "      shutil.copytree(source, destination)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Obtain the ground truth values for patients who have an abnormality (predicted by atleast two radiologists)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2SoI47QYc95"
      },
      "outputs": [],
      "source": [
        "ground_truth = pd.read_csv('/content/gdrive/MyDrive/FYP/reads.csv')\n",
        "verdict = [\n",
        "           'ICH',\n",
        "           'IPH',\n",
        "           'IVH',\n",
        "           'SDH',\n",
        "           'EDH',\n",
        "           'SAH',\n",
        "           'BleedLocation-Left',\n",
        "           'BleedLocation-Right',\n",
        "           'ChronicBleed',\n",
        "           'Fracture',\n",
        "           'CalvarialFracture',\n",
        "           'OtherFracture',\n",
        "           'MassEffect',\n",
        "           'MidlineShift'\n",
        "           ]\n",
        "for col in verdict:\n",
        "  ground_truth[col] = ground_truth[f'R1:{col}'] + ground_truth[f'R2:{col}'] + ground_truth[f'R3:{col}']\n",
        "  ground_truth[col] = ground_truth[col].apply(lambda x: int(x >=2))\n",
        "ground_truth.to_csv('/content/gdrive/MyDrive/FYP/ground_truth.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Storing the meta-information of the slices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HrUkYA1CrIE",
        "outputId": "e440ae76-c8ce-4558-9628-9c9c90f2c6b8"
      },
      "outputs": [],
      "source": [
        "meta_cols = [\n",
        "             'SOPInstanceUID',\n",
        "             'PatientID',\n",
        "             'SeriesInstanceUID',\n",
        "             'StudyInstanceUID',\n",
        "             ] # required metadata information\n",
        "information = []\n",
        "data_path = '/content/gdrive/MyDrive/FYP/Dataset'\n",
        "for path, subdirs, files in os.walk(data_path):\n",
        "  for name in tqdm(files): \n",
        "    current_dict = {}\n",
        "    current_dict = {col: \"\" for col in meta_cols}\n",
        "    current_dict['subfolder'] = \"\"  \n",
        "    if 'dcm' in name:\n",
        "      dicom_object = pydicom.dcmread(os.path.join(path, name))\n",
        "      for col in meta_cols: \n",
        "        current_dict[col] = str(getattr(dicom_object, col))\n",
        "      current_dict['subfolder'] = path.split('/')[-1]\n",
        "      information.append(current_dict)    \n",
        "\n",
        "df = pd.DataFrame.from_dict(information)\n",
        "df.to_csv('/content/gdrive/MyDrive/FYP/required_dicom_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Save the slices saved as a dicom file as an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Je7rvpbgcEXB"
      },
      "outputs": [],
      "source": [
        "def transform_to_hu(medical_image, image):\n",
        "    intercept = medical_image.RescaleIntercept\n",
        "    slope = medical_image.RescaleSlope\n",
        "    hu_image = image * slope + intercept\n",
        "    return hu_image\n",
        "\n",
        "def window_image(image, window_center, window_width):\n",
        "    img_min = window_center - window_width // 2\n",
        "    img_max = window_center + window_width // 2\n",
        "    window_image = image.copy()\n",
        "    window_image[window_image < img_min] = img_min\n",
        "    window_image[window_image > img_max] = img_max\n",
        "    \n",
        "    return window_image\n",
        "\n",
        "meta_cols = ['SOPInstanceUID',] # required metadata information\n",
        "data_path = '/content/gdrive/MyDrive/FYP/Dataset'\n",
        "for path, subdirs, files in os.walk(data_path):\n",
        "  for name in tqdm(files): \n",
        "    if 'dcm' in name:\n",
        "      medical_image = pydicom.read_file(os.path.join(path, name))\n",
        "      for col in meta_cols: \n",
        "        current_dicom =  str(getattr(dicom_object, col))\n",
        "      image = medical_image.pixel_array\n",
        "      hu_image = transform_to_hu(medical_image, image)\n",
        "      brain_image = window_image(hu_image, 40, 80)\n",
        "      destination = f'/content/gdrive/MyDrive/FYP/required_dicoms/{current_dicom}.png'\n",
        "      mpimg.imsave((destination), brain_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib5tsTX8hFcu",
        "outputId": "ba04b112-815a-4c30-b775-fba80e4f7268"
      },
      "outputs": [],
      "source": [
        "reqd = pd.read_csv('/content/gdrive/MyDrive/FYP/required_dicom_data.csv')\n",
        "abnormal = pd.read_csv('/content/gdrive/MyDrive/FYP/3_Extrapolation_to_Selected_Series.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7AdiDNqhooO",
        "outputId": "5c148565-6d4e-45b5-f628-9a92566f278c"
      },
      "outputs": [],
      "source": [
        "data = pd.merge(abnormal, reqd, how='right', on=['SOPInstanceUID', 'SeriesInstanceUID','StudyInstanceUID'])\n",
        "data = data[~data['PatientID'].isin(other_abnormalities)]\n",
        "data['abnormal'] = data['labelName'].apply(lambda x: 1 if (type(x) == str) else 0)\n",
        "data.to_csv('/content/gdrive/MyDrive/FYP/master_key_hemo_and_normal.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Extracting patient list for normal/hemorrhage/others abnormalities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.read_csv('./reads.csv')\n",
        "\n",
        "cols = ['ICH', 'IPH', 'IVH', 'SDH', 'EDH',\n",
        "       'SAH', 'BleedLocation-Left', 'BleedLocation-Right',\n",
        "       'ChronicBleed', 'Fracture', 'CalvarialFracture',\n",
        "       'OtherFracture', 'MassEffect', 'MidlineShift']\n",
        "       \n",
        "cols = ['R1:ICH', 'R1:IPH', 'R1:IVH', 'R1:SDH', 'R1:EDH',\n",
        "       'R1:SAH', 'R1:BleedLocation-Left', 'R1:BleedLocation-Right',\n",
        "       'R1:ChronicBleed', 'R1:Fracture', 'R1:CalvarialFracture',\n",
        "       'R1:OtherFracture', 'R1:MassEffect', 'R1:MidlineShift', 'R2:ICH',\n",
        "       'R2:IPH', 'R2:IVH', 'R2:SDH', 'R2:EDH', 'R2:SAH',\n",
        "       'R2:BleedLocation-Left', 'R2:BleedLocation-Right', 'R2:ChronicBleed',\n",
        "       'R2:Fracture', 'R2:CalvarialFracture', 'R2:OtherFracture',\n",
        "       'R2:MassEffect', 'R2:MidlineShift', 'R3:ICH', 'R3:IPH', 'R3:IVH',\n",
        "       'R3:SDH', 'R3:EDH', 'R3:SAH', 'R3:BleedLocation-Left',\n",
        "       'R3:BleedLocation-Right', 'R3:ChronicBleed', 'R3:Fracture',\n",
        "       'R3:CalvarialFracture', 'R3:OtherFracture', 'R3:MassEffect',\n",
        "       'R3:MidlineShift']\n",
        "\n",
        "extr = data\n",
        "for column in cols:\n",
        "    extr = extr[extr[column] == 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "normal_scans = list(extr['name'])\n",
        "import pickle as pkl\n",
        "with open('./normal_scans_list.pkl', 'wb') as f:\n",
        "    pkl.dump(normal_scans, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "abnormal_scans = list(set(data['name']) - set(normal_scans))\n",
        "with open('./abnormal_scans_list.pkl', 'wb') as f:\n",
        "    pkl.dump(abnormal_scans, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Split the dataset into train and test set and store the files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Binary Classification Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmX8UoUAQ7hy"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/FYP/master_key_hemo_and_normal.csv')\n",
        "normal = list(df[df['abnormal'] == 0]['SOPInstanceUID'].values)\n",
        "abnormal = list(df[df['abnormal'] == 1]['SOPInstanceUID'].values)\n",
        "\n",
        "root_dir = './binary_classification_data/' # data root path\n",
        "classes_dir = ['normal', 'abnormal'] #total labels\n",
        "\n",
        "val_ratio = 0.20\n",
        "test_ratio = 0.15\n",
        "\n",
        "for cls in classes_dir:\n",
        "    os.makedirs(root_dir +'train/' + cls)\n",
        "    os.makedirs(root_dir +'val/' + cls)\n",
        "    os.makedirs(root_dir +'test/' + cls)\n",
        "\n",
        "src = './required_dicoms'\n",
        "allFileNames = os.listdir(src)\n",
        "allFileNames = [file[:-4] for file in allFileNames]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.shuffle(allFileNames)\n",
        "train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),\n",
        "                                                          [int(len(allFileNames)* (1 - (val_ratio + test_ratio))), \n",
        "                                                           int(len(allFileNames)* (1 - test_ratio))])\n",
        "\n",
        "\n",
        "train_normal = [name for name in train_FileNames.tolist() if name in normal]\n",
        "train_abnormal = [name for name in train_FileNames.tolist() if name in abnormal]\n",
        "\n",
        "validation_normal = [name for name in val_FileNames.tolist() if name in normal]\n",
        "validation_abnormal = [name for name in val_FileNames.tolist() if name in abnormal]\n",
        "\n",
        "test_normal = [name for name in test_FileNames.tolist() if name in normal]\n",
        "test_abnormal = [name for name in test_FileNames.tolist() if name in abnormal]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "finals = train_normal + train_abnormal + validation_normal + validation_abnormal + test_normal + test_abnormal\n",
        "all_missing = [name for name in allFileNames if name not in finals]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_normal = [src+'/'+ name for name in train_normal]\n",
        "train_abnormal = [src+'/'+ name for name in train_abnormal]\n",
        "\n",
        "validation_normal = [src+'/'+ name for name in validation_normal]\n",
        "validation_abnormal = [src+'/'+ name for name in validation_abnormal ]\n",
        "\n",
        "test_normal = [src+'/'+ name for name in test_normal]\n",
        "test_abnormal = [src+'/'+ name for name in test_abnormal]\n",
        "\n",
        "print('Total images: ', len(allFileNames))\n",
        "\n",
        "print('Training (normal): ', len(train_normal))\n",
        "print('Validation (normal): ', len(validation_normal))\n",
        "print('Testing (normal): ', len(test_normal))\n",
        "\n",
        "print('Training (abnormal): ', len(train_abnormal))\n",
        "print('Validation (abnormal): ', len(validation_abnormal))\n",
        "print('Testing (abnormal): ', len(test_abnormal))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for name in tqdm(train_normal):\n",
        "    shutil.copy(name + '.png', root_dir +'train/' + 'normal')\n",
        "\n",
        "for name in tqdm(validation_normal):\n",
        "    shutil.copy(name + '.png', root_dir +'val/' + 'normal')\n",
        "\n",
        "for name in tqdm(test_normal):\n",
        "    shutil.copy(name + '.png', root_dir +'test/' + 'normal')\n",
        "\n",
        "for name in tqdm(train_abnormal):\n",
        "    shutil.copy(name + '.png', root_dir +'train/' + 'abnormal')\n",
        "\n",
        "for name in tqdm(validation_abnormal):\n",
        "    shutil.copy(name + '.png', root_dir +'val/' + 'abnormal')\n",
        "\n",
        "for name in tqdm(test_abnormal):\n",
        "    shutil.copy(name + '.png', root_dir +'test/' + 'abnormal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Multi-Class Classification Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/FYP/master_key_hemo_and_normal.csv')\n",
        "df['what_abnormality'] = df['labelName'].fillna('Normal')\n",
        "df.to_csv('/content/gdrive/MyDrive/FYP/master_key_multiClass.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "root_dir = './multi_classification_data/' # data root path\n",
        "classes_dir = list(set(df['what_abnormality']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        " #total labels\n",
        "val_ratio = 0.20\n",
        "test_ratio = 0.15\n",
        "\n",
        "for cls in classes_dir:\n",
        "    os.makedirs(root_dir +'train/' + cls)\n",
        "    os.makedirs(root_dir +'val/' + cls)\n",
        "    os.makedirs(root_dir +'test/' + cls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "src = './required_dicoms'\n",
        "allFileNames = os.listdir(src)\n",
        "allFileNames = [file[:-4] for file in allFileNames]\n",
        "\n",
        "Intraventricular = df[df['what_abnormality'] == 'Intraventricular']['SOPInstanceUID'].tolist()\n",
        "Normal = df[df['what_abnormality'] == 'Normal']['SOPInstanceUID'].tolist()\n",
        "Subarachnoid = df[df['what_abnormality'] == 'Subarachnoid']['SOPInstanceUID'].tolist()\n",
        "Chronic = df[df['what_abnormality'] == 'Chronic']['SOPInstanceUID'].tolist()\n",
        "Intraparenchymal = df[df['what_abnormality'] == 'Intraparenchymal']['SOPInstanceUID'].tolist()\n",
        "Epidural = df[df['what_abnormality'] == 'Epidural']['SOPInstanceUID'].tolist()\n",
        "Subdural = df[df['what_abnormality'] == 'Subdural']['SOPInstanceUID'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.shuffle(allFileNames)\n",
        "train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),\n",
        "                                                          [int(len(allFileNames)* (1 - (val_ratio + test_ratio))), \n",
        "                                                           int(len(allFileNames)* (1 - test_ratio))])\n",
        "\n",
        "\n",
        "train_Intraventricular = [name for name in train_FileNames.tolist() if name in Intraventricular]\n",
        "train_Normal = [name for name in train_FileNames.tolist() if name in Normal]\n",
        "train_Subarachnoid = [name for name in train_FileNames.tolist() if name in Subarachnoid]\n",
        "train_Chronic = [name for name in train_FileNames.tolist() if name in Chronic]\n",
        "train_Intraparenchymal = [name for name in train_FileNames.tolist() if name in Intraparenchymal]\n",
        "train_Epidural = [name for name in train_FileNames.tolist() if name in Epidural]\n",
        "train_Subdural = [name for name in train_FileNames.tolist() if name in Subdural]\n",
        "\n",
        "validation_Intraventricular = [name for name in val_FileNames.tolist() if name in Intraventricular]\n",
        "validation_Normal = [name for name in val_FileNames.tolist() if name in Normal]\n",
        "validation_Subarachnoid = [name for name in val_FileNames.tolist() if name in Subarachnoid]\n",
        "validation_Chronic = [name for name in val_FileNames.tolist() if name in Chronic]\n",
        "validation_Intraparenchymal = [name for name in val_FileNames.tolist() if name in Intraparenchymal]\n",
        "validation_Epidural = [name for name in val_FileNames.tolist() if name in Epidural]\n",
        "validation_Subdural = [name for name in val_FileNames.tolist() if name in Subdural]\n",
        "\n",
        "test_Intraventricular = [name for name in test_FileNames.tolist() if name in Intraventricular]\n",
        "test_Normal = [name for name in test_FileNames.tolist() if name in Normal]\n",
        "test_Subarachnoid = [name for name in test_FileNames.tolist() if name in Subarachnoid]\n",
        "test_Chronic = [name for name in test_FileNames.tolist() if name in Chronic]\n",
        "test_Intraparenchymal = [name for name in test_FileNames.tolist() if name in Intraparenchymal]\n",
        "test_Epidural = [name for name in test_FileNames.tolist() if name in Epidural]\n",
        "test_Subdural = [name for name in test_FileNames.tolist() if name in Subdural]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_Intraventricular = [src+'/'+ name for name in train_Intraventricular]\n",
        "train_Normal = [src+'/'+ name for name in train_Normal]\n",
        "train_Subarachnoid = [src+'/'+ name for name in train_Subarachnoid]\n",
        "train_Chronic = [src+'/'+ name for name in train_Chronic]\n",
        "train_Intraparenchymal = [src+'/'+ name for name in train_Intraparenchymal]\n",
        "train_Epidural = [src+'/'+ name for name in train_Epidural]\n",
        "train_Subdural = [src+'/'+ name for name in train_Subdural]\n",
        "\n",
        "validation_Intraventricular = [src+'/'+ name for name in validation_Intraventricular]\n",
        "validation_Normal = [src+'/'+ name for name in validation_Normal]\n",
        "validation_Subarachnoid = [src+'/'+ name for name in validation_Subarachnoid]\n",
        "validation_Chronic = [src+'/'+ name for name in validation_Chronic]\n",
        "validation_Intraparenchymal = [src+'/'+ name for name in validation_Intraparenchymal]\n",
        "validation_Epidural = [src+'/'+ name for name in validation_Epidural]\n",
        "validation_Subdural = [src+'/'+ name for name in validation_Subdural]\n",
        "\n",
        "test_Intraventricular = [src+'/'+ name for name in test_Intraventricular]\n",
        "test_Normal = [src+'/'+ name for name in test_Normal]\n",
        "test_Subarachnoid = [src+'/'+ name for name in test_Subarachnoid]\n",
        "test_Chronic = [src+'/'+ name for name in test_Chronic]\n",
        "test_Intraparenchymal = [src+'/'+ name for name in test_Intraparenchymal]\n",
        "test_Epidural = [src+'/'+ name for name in test_Epidural]\n",
        "test_Subdural = [src+'/'+ name for name in test_Subdural]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Total images: ', len(allFileNames))\n",
        "\n",
        "print('Training (Intraventricular): ', len(train_Intraventricular))\n",
        "print('Validation (Intraventricular): ', len(validation_Intraventricular))\n",
        "print('Testing (Intraventricular): ', len(test_Intraventricular))\n",
        "\n",
        "print('Training (Normal): ', len(train_Normal))\n",
        "print('Validation (Normal): ', len(validation_Normal))\n",
        "print('Testing (Normal): ', len(test_Normal))\n",
        "\n",
        "print('Training (Subarachnoid): ', len(train_Subarachnoid))\n",
        "print('Validation (Subarachnoid): ', len(validation_Subarachnoid))\n",
        "print('Testing (Subarachnoid): ', len(test_Subarachnoid))\n",
        "\n",
        "print('Training (Chronic): ', len(train_Chronic))\n",
        "print('Validation (Chronic): ', len(validation_Chronic))\n",
        "print('Testing (Chronic): ', len(test_Chronic))\n",
        "\n",
        "print('Training (Intraparenchymal): ', len(train_Intraparenchymal))\n",
        "print('Validation (Intraparenchymal): ', len(validation_Intraparenchymal))\n",
        "print('Testing (Intraparenchymal): ', len(test_Intraparenchymal))\n",
        "\n",
        "print('Training (Epidural): ', len(train_Epidural))\n",
        "print('Validation (Epidural): ', len(validation_Epidural))\n",
        "print('Testing (Epidural): ', len(test_Epidural))\n",
        "\n",
        "print('Training (Subdural): ', len(train_Subdural))\n",
        "print('Validation (Subdural): ', len(validation_Subdural))\n",
        "print('Testing (Subdural): ', len(test_Subdural))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for name in tqdm(train_Intraventricular):\n",
        "    shutil.copy(name + '.png', root_dir +'train/' + 'Intraventricular')\n",
        "\n",
        "for name in tqdm(validation_Intraventricular):\n",
        "    shutil.copy(name + '.png', root_dir +'val/' + 'Intraventricular')\n",
        "\n",
        "for name in tqdm(test_Intraventricular):\n",
        "    shutil.copy(name + '.png', root_dir +'test/' + 'Intraventricular')\n",
        "\n",
        "for name in tqdm(train_Normal):\n",
        "    shutil.copy(name + '.png', root_dir +'train/' + 'Normal')\n",
        "\n",
        "for name in tqdm(validation_Normal):\n",
        "    shutil.copy(name + '.png', root_dir +'val/' + 'Normal')\n",
        "\n",
        "for name in tqdm(test_Normal):\n",
        "    shutil.copy(name + '.png', root_dir +'test/' + 'Normal')\n",
        "\n",
        "for name in tqdm(train_Subarachnoid):\n",
        "    shutil.copy(name + '.png', root_dir +'train/' + 'Subarachnoid')\n",
        "\n",
        "for name in tqdm(validation_Subarachnoid):\n",
        "    shutil.copy(name + '.png', root_dir +'val/' + 'Subarachnoid')\n",
        "\n",
        "for name in tqdm(test_Subarachnoid):\n",
        "    shutil.copy(name + '.png', root_dir +'test/' + 'Subarachnoid')\n",
        "\n",
        "for name in tqdm(train_Chronic):\n",
        "    shutil.copy(name + '.png', root_dir +'train/' + 'Chronic')\n",
        "\n",
        "for name in tqdm(validation_Chronic):\n",
        "    shutil.copy(name + '.png', root_dir +'val/' + 'Chronic')\n",
        "\n",
        "for name in tqdm(test_Chronic):\n",
        "    shutil.copy(name + '.png', root_dir +'test/' + 'Chronic')\n",
        "\n",
        "for name in tqdm(train_Intraparenchymal):\n",
        "    shutil.copy(name + '.png', root_dir +'train/' + 'Intraparenchymal')\n",
        "\n",
        "for name in tqdm(validation_Intraparenchymal):\n",
        "    shutil.copy(name + '.png', root_dir +'val/' + 'Intraparenchymal')\n",
        "\n",
        "for name in tqdm(test_Intraparenchymal):\n",
        "    shutil.copy(name + '.png', root_dir +'test/' + 'Intraparenchymal')\n",
        "\n",
        "for name in tqdm(train_Epidural):\n",
        "    shutil.copy(name + '.png', root_dir +'train/' + 'Epidural')\n",
        "\n",
        "for name in tqdm(validation_Epidural):\n",
        "    shutil.copy(name + '.png', root_dir +'val/' + 'Epidural')\n",
        "\n",
        "for name in tqdm(test_Epidural):\n",
        "    shutil.copy(name + '.png', root_dir +'test/' + 'Epidural')\n",
        "\n",
        "for name in tqdm(train_Subdural):\n",
        "    shutil.copy(name + '.png', root_dir +'train/' + 'Subdural')\n",
        "\n",
        "for name in tqdm(validation_Subdural):\n",
        "    shutil.copy(name + '.png', root_dir +'val/' + 'Subdural')\n",
        "\n",
        "for name in tqdm(test_Subdural):\n",
        "    shutil.copy(name + '.png', root_dir +'test/' + 'Subdural')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "FYP_FileHandling",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
